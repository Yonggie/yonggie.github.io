---
title: "Navigation with VLM framework: Go to Any Language"
collection: publications
category: manuscripts
permalink: /publication/202409-icra
excerpt: 'Navigating towards fully open language goals and exploring open scenes in a manner akin to human exploration have always posed significant challenges. Recently, Vision Large Language Models (VLMs) have demonstrated remarkable capabilities in reasoning with both language and visual data. While many works have focused on leveraging VLMs for navigation in open scenes and with open vocabularies, these efforts often fall short of fully utilizing the potential of VLMs or require substantial computational resources.
We introduce Navigation with VLM (NavVLM), a framework that harnesses equipment-level VLMs to enable agents to navigate towards any language goal specific or non-specific in open scenes, emulating human exploration behaviors without any prior training. The agent leverages the VLM as its cognitive core to perceive environmental information based on any language goal and constantly provides exploration guidance during navigation until it reaches the target location or area.
Our framework not only achieves state-of-the-art performance in Success Rate (SR) and Success weighted by Path Length (SPL) in traditional specific goal settings but also extends the navigation capabilities to any open-set language goal. We evaluate NavVLM in richly detailed environments from the Matterport 3D (MP3D), Habitat Matterport 3D (HM3D), and Gibson datasets within the Habitat simulator. With  the power of VLMs, navigation has entered a new era.'
date: 2024-09-15
venue: ICRA'25 (Under Review)
slidesurl: 'https://link.springer.com/chapter/10.1007/978-3-031-40292-0_5'
paperurl: 'https://link.springer.com/chapter/10.1007/978-3-031-40292-0_5'
citation: '**Zecheng Yin**, Chonghao Cheng, Yinghong Liao, Zhihao Yuan, Shuguang Cui, Zhen Li. , Navigation with VLM framework: Go to Any Language, ICRA'25 (Under review)'
---
